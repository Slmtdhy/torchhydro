{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cc5816-be5e-482e-95cc-80bca2882ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714650e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(y_dir, x_dir, gages, gages1, flow, seq_length, input_size):\n",
    "    X = np.zeros((flow.shape[0], seq_length, input_size))\n",
    "    for i, basin in gages.iterrows():\n",
    "        df2 = pd.read_csv(os.path.join(y_dir, basin['STAID'] + '_Q_Month.txt'),\n",
    "                         skiprows=38, delimiter=';', parse_dates=[0], dayfirst=False, encoding='cp1252')\n",
    "        # 判断是否有至少一行 'Original' column 中的值为 -999.000\n",
    "        if -999.000 in df2[' Original'].values:\n",
    "            # 使用 'Calculated' 数据替换 'Original' 数据中值为 -999.000 的行\n",
    "            df2.loc[df2[' Original'] == -999.000, ' Original'] = df2.loc[df2[' Original'] == -999.000, ' Calculated']\n",
    "        df2 = df2.drop(columns=[' Calculated'])\n",
    "        # 将时间列转换为日期时间格式\n",
    "        df2['YYYY-MM-DD'] = pd.to_datetime(df2['YYYY-MM-DD'])\n",
    "        df2['Y'] = df2['YYYY-MM-DD'].dt.year\n",
    "        df2['M'] = df2['YYYY-MM-DD'].dt.month\n",
    "        # 筛选出时间在1951年之后的行\n",
    "        df2 = df2[(df2['YYYY-MM-DD'].dt.year >= 1951) & (df2['YYYY-MM-DD'].dt.year <= 2020)].reset_index(drop=True)\n",
    "        beg_year = df2['YYYY-MM-DD'].dt.year.iloc[0]\n",
    "        beg_month = df2['YYYY-MM-DD'].dt.month.iloc[0]\n",
    "        df1 = pd.read_csv(os.path.join(x_dir, basin['STAID'] + '.csv'), parse_dates=[0])\n",
    "        # 将时间列转换为日期时间格式\n",
    "        df1['time'] = pd.to_datetime(df1['time'])\n",
    "        if (df2['YYYY-MM-DD'].dt.year.iloc[0] == 1951) and (df2['YYYY-MM-DD'].dt.month.iloc[0] == 1):\n",
    "            # 获取时间列\n",
    "            time_column = df2['YYYY-MM-DD']\n",
    "            # 使用isin方法过滤第二个CSV文件的数据\n",
    "            df1 = df1[df1['time'].isin(time_column)]\n",
    "        else:\n",
    "            df1['Y'] = df1['time'].dt.year\n",
    "            df1['M'] = df1['time'].dt.month\n",
    "            # 根据逆推公式获取预测变量的开始年份和月份\n",
    "            pred_var_beg_year = beg_year - abs(seq_length - beg_month) // 12 - 1\n",
    "            pred_var_beg_month = 13 - abs(seq_length - beg_month) % 12\n",
    "            # 筛选出时间在径流数据起始时间之后的行\n",
    "            df1 = df1[\n",
    "                ((df1['Y'] > pred_var_beg_year) | ((df1['Y'] == pred_var_beg_year) & (df1['M'] >= pred_var_beg_month)))]\n",
    "        df1 = df1.iloc[:, 1:input_size+1].reset_index(drop=True)\n",
    "        basin_idx = flow['basin'] == i\n",
    "        delta = flow['delta'][basin_idx]\n",
    "        idx_rep = np.repeat(delta, seq_length)\n",
    "        if basin['STAID'] in gages1['STAID'].values:\n",
    "            idx_sub = np.tile(np.arange(0, seq_length), delta.shape[0])\n",
    "            dx = idx_rep + idx_sub\n",
    "        else:\n",
    "            idx_sub = np.tile(np.arange(0, seq_length)[::-1], delta.shape[0])\n",
    "            dx = idx_rep - idx_sub  \n",
    "        X[basin_idx] = np.array(df1.iloc[dx]).reshape(X[basin_idx].shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42dfd51f-eb1d-4cc0-b39d-77ae00f31179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dynamic(Dataset):\n",
    "\n",
    "    # load the dataset\n",
    "    def __init__(self, gages_dir, gages_dir1, xd_dir, xg_dir, y_dir, seq_length, input_size_dyn, input_size_glo, output_size):\n",
    "        flow = pd.DataFrame()\n",
    "        gages = pd.read_excel(gages_dir, usecols=['STAID'], dtype=str)  # 获取站点编号\n",
    "        gages1 = pd.read_excel(gages_dir1, usecols=['STAID'], dtype=str)\n",
    "        gages['num_months'] = -1\n",
    "\n",
    "        for i, basin in gages.iterrows():\n",
    "            df = pd.read_csv(os.path.join(y_dir, basin['STAID'] + '_Q_Month.txt'),\n",
    "                             skiprows=38, delimiter=';', parse_dates=[0], dayfirst=False, encoding='cp1252')\n",
    "            # 判断是否有至少一行 'Original' column 中的值为 -999.000\n",
    "            if -999.000 in df[' Calculated'].values:\n",
    "                # 使用 'Original' 数据替换 'Calculated' 数据中值为 -999.000 的行\n",
    "                df.loc[df[' Calculated'] == -999.000, ' Calculated'] = df.loc[df[' Calculated'] == -999.000, ' Original']\n",
    "            df = df.drop(columns=[' Original'])\n",
    "            # Set -999.000 values to NaN\n",
    "            df[' Calculated'] = df[' Calculated'].replace(-999.000, np.nan)\n",
    "            # 将时间列转换为日期时间格式\n",
    "            df['YYYY-MM-DD'] = pd.to_datetime(df['YYYY-MM-DD'])\n",
    "            df['Y'] = df['YYYY-MM-DD'].dt.year\n",
    "            df['M'] = df['YYYY-MM-DD'].dt.month\n",
    "            # 筛选出时间在1951年之后的行\n",
    "            df = df[(df['YYYY-MM-DD'].dt.year >= 1951) & (df['YYYY-MM-DD'].dt.year <= 2020)].reset_index(drop=True)\n",
    "            beg_year = df['YYYY-MM-DD'].dt.year.iloc[0]\n",
    "            beg_month = df['YYYY-MM-DD'].dt.month.iloc[0]\n",
    "            # # 对前 80% 行进行线性插值\n",
    "            # num_rows_to_interpolate = np.rint(0.8 * (df.shape[0])).astype('int')\n",
    "            # df.iloc[:num_rows_to_interpolate] = df.iloc[:num_rows_to_interpolate].interpolate(method='linear')\n",
    "            if (df['YYYY-MM-DD'].dt.year.iloc[0] == 1951) and (df['YYYY-MM-DD'].dt.month.iloc[0] == 1):\n",
    "                flow_beg = np.searchsorted((df['Y'] == beg_year + (beg_month + seq_length - 2) // 12) &\n",
    "                                           (df['M'] >= (beg_month + seq_length - 2) % 12 + 1) |\n",
    "                                           (df['Y'] >= beg_year + (beg_month + seq_length - 2) // 12 + 1), True)\n",
    "            else:\n",
    "                flow_beg = 0\n",
    "\n",
    "            if flow_beg < df.shape[0]:\n",
    "                df['delta'] = (df['Y'] - beg_year) * 12 + df['M'] - beg_month\n",
    "                flow = pd.concat([flow, df.iloc[flow_beg:]], axis=0, ignore_index=True)\n",
    "                gages.loc[i, 'num_months'] = df.iloc[flow_beg:].shape[0]\n",
    "\n",
    "        gages = gages[gages['num_months'] != -1]\n",
    "        gages['t'] = np.cumsum(gages['num_months'])\n",
    "        gages['s'] = gages['t'].shift(1, fill_value=0)\n",
    "        flow['basin'] = np.repeat(np.arange(0, gages.shape[0], dtype=int), gages['t'] - gages['s'])\n",
    "        Xg = process_data(y_dir, xg_dir, gages, gages1, flow, seq_length , input_size_glo)\n",
    "        Xd = process_data(y_dir, xd_dir, gages, gages1, flow, seq_length , input_size_dyn)\n",
    "        y = flow[' Calculated'].values.reshape((flow.shape[0], output_size))\n",
    "        basin = flow['basin'].values\n",
    "        date_integers = flow['YYYY-MM-DD'].map(lambda date: date.toordinal())\n",
    "        \n",
    "        \n",
    "#         # 找到非空 y 值的索引\n",
    "#         non_empty_indices = np.where(~np.isnan(y.flatten()))[0]\n",
    "         \n",
    "#         # 提取非空 y 值对应的 x 值\n",
    "#         Xg = Xg[non_empty_indices]\n",
    "#         Xd = Xd[non_empty_indices]\n",
    "#         y = y[non_empty_indices]\n",
    "#         basin = basin[non_empty_indices]\n",
    "        # ensure inputs and target has the right dtype\n",
    "        self.gages = gages\n",
    "        self.Xg = Xg.astype('float32')\n",
    "        self.Xd = Xd.astype('float32')\n",
    "        self.y = y.astype('float32')\n",
    "        self.basin = basin.astype('int32')\n",
    "        self.time = date_integers.values.astype('int32')\n",
    "\n",
    "        # prevent subclass from calling Dynamic.normalize\n",
    "        self.norm = None\n",
    "        if isinstance(self, Dynamic):\n",
    "            self.normalize()\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return [len(self.Xd),len(self.Xg)]\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.Xd[idx], self.Xg[idx], self.y[idx], self.basin[idx], self.time[idx]]\n",
    "\n",
    "    # normalization\n",
    "    def normalize(self, norm=None):\n",
    "        # cannot normalize twice\n",
    "        if self.norm is not None:\n",
    "            return self.norm\n",
    "\n",
    "        # scaler of Xd\n",
    "        if norm is None:\n",
    "            Xd_mean = self.Xd.reshape(-1, self.Xd.shape[-1]).mean(axis=0)\n",
    "            Xd_std = self.Xd.reshape(-1, self.Xd.shape[-1]).std(axis=0)\n",
    "        else:\n",
    "            Xd_mean = norm[0]\n",
    "            Xd_std = norm[1]\n",
    "        # scaler of X\n",
    "        if norm is None:\n",
    "            Xg_mean = self.Xg.reshape(-1, self.Xg.shape[-1]).mean(axis=0)\n",
    "            Xg_std = self.Xg.reshape(-1, self.Xg.shape[-1]).std(axis=0)\n",
    "        else:\n",
    "            Xg_mean = norm[2]\n",
    "            Xg_std = norm[3]\n",
    "        # scaler of y\n",
    "        if norm is None:\n",
    "            y_mean = 0\n",
    "            # y_std = self.y.max(axis=0) - y_mean #有空值不可用\n",
    "            y_std = np.nanmax(self.y, axis=0) - y_mean\n",
    "        else:\n",
    "            y_mean = norm[4]\n",
    "            y_std = norm[5]\n",
    "\n",
    "        # normalize X and y\n",
    "        self.Xd = (self.Xd - Xd_mean) / Xd_std\n",
    "        self.Xg = (self.Xg - Xg_mean) / Xg_std\n",
    "        self.y = (self.y - y_mean) / y_std\n",
    "        self.norm = (Xd_mean, Xd_std, Xg_mean, Xg_std, y_mean, y_std)\n",
    "    \n",
    "        return self.norm\n",
    "    \n",
    "\n",
    "class WithStatic(Dynamic):\n",
    "    # load the dataset\n",
    "    def __init__(self, gages_dir, gages_dir1, S_dir, xd_dir,  xg_dir, y_dir, seq_length, \n",
    "                 input_size_dyn, input_size_glo, input_size_sta, output_size):\n",
    "        super().__init__( gages_dir, gages_dir1, xd_dir, xg_dir, y_dir, seq_length, \n",
    "                         input_size_dyn, input_size_glo, output_size)\n",
    "        df_sta = pd.read_csv(S_dir, dtype={'STAID': str})\n",
    "        S = pd.merge(self.gages, df_sta, how='left', on='STAID').iloc[:, -input_size_sta:]\n",
    "        self.S = S.values[self.basin].astype('float32')\n",
    "        # normalization static\n",
    "        self.normalize()\n",
    "    def __getitem__(self, idx):\n",
    "        return [(self.S[idx], self.Xd[idx], self.Xg[idx]), self.y[idx], self.basin[idx], self.time[idx]]\n",
    "    def normalize(self, norm=None):\n",
    "        # cannot normalize twice\n",
    "        if self.norm is not None and len(self.norm) == 8:\n",
    "            return self.norm\n",
    "        if self.norm is None:\n",
    "            if norm is None:\n",
    "                super().normalize()\n",
    "            else:\n",
    "                super().normalize(norm=norm[2:])\n",
    "        if hasattr(self, 'S'):\n",
    "            if norm is None:\n",
    "                S_mean = self.S.mean(axis=0)\n",
    "                S_std = self.S.std(axis=0)\n",
    "            else:\n",
    "                S_mean = norm[0]\n",
    "                S_std = norm[1]\n",
    "            self.S = (self.S - S_mean) / S_std\n",
    "            self.norm = (S_mean, S_std, *self.norm)\n",
    "        return self.norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf81c2d6-296f-41f4-b17c-2a2f4994c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WithStatic(Dynamic):\n",
    "    # load the dataset\n",
    "    def __init__(self, gages_dir, gages_dir1, S_dir, xd_dir,  xg_dir, y_dir, seq_length, \n",
    "                 input_size_dyn, input_size_glo, input_size_sta, output_size):\n",
    "        super().__init__( gages_dir, gages_dir1, xd_dir, xg_dir, y_dir, seq_length, \n",
    "                         input_size_dyn, input_size_glo, output_size)\n",
    "        df_sta = pd.read_csv(S_dir, dtype={'STAID': str})\n",
    "        S = pd.merge(self.gages, df_sta, how='left', on='STAID').iloc[:, -input_size_sta:]\n",
    "        self.S = S.values[self.basin].astype('float32')\n",
    "        # normalization static\n",
    "        self.normalize()\n",
    "    def __getitem__(self, idx):\n",
    "        return [(self.S[idx], self.Xd[idx], self.Xg[idx]), self.y[idx], self.basin[idx], self.time[idx]]\n",
    "    def normalize(self, norm=None):\n",
    "        # cannot normalize twice\n",
    "        if self.norm is not None and len(self.norm) == 8:\n",
    "            return self.norm\n",
    "        if self.norm is None:\n",
    "            if norm is None:\n",
    "                super().normalize()\n",
    "            else:\n",
    "                super().normalize(norm=norm[2:])\n",
    "        if hasattr(self, 'S'):\n",
    "            if norm is None:\n",
    "                S_mean = self.S.mean(axis=0)\n",
    "                S_std = self.S.std(axis=0)\n",
    "            else:\n",
    "                S_mean = norm[0]\n",
    "                S_std = norm[1]\n",
    "            self.S = (self.S - S_mean) / S_std\n",
    "            self.norm = (S_mean, S_std, *self.norm)\n",
    "        return self.norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf7e514-c39a-43dd-ab2e-ec045c1f0c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S_dir = 'data/balstm/grdc_attributes1.csv'\n",
    "gages_dir = 'data/balstm/basin_list.xlsx'\n",
    "gages_dir1 = 'data/balstm/basin_def.xlsx'\n",
    "y_dir = 'data/balstm/runoff_data/txt'\n",
    "xd_dir = 'data/balstm/forcing_t'\n",
    "xg_dir = 'data/balstm/global_data/output'\n",
    "seq_length = 12\n",
    "input_size_dyn = 13\n",
    "input_size_glo = 106\n",
    "output_size = 1\n",
    "input_size_sta = 195\n",
    "dataset = WithStatic(gages_dir, gages_dir1, S_dir, xd_dir,  xg_dir, y_dir, seq_length, \n",
    "                 input_size_dyn, input_size_glo, input_size_sta, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fa58be1-c026-4f40-bbfa-2fda42c48491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120281, 195)\n",
      "(120281, 12, 13)\n",
      "(120281, 12, 106)\n",
      "(120281, 1)\n",
      "(120281,)\n",
      "(120281,)\n",
      "[712557 712588 712619 ... 726741 726772 726802]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.S.shape)\n",
    "print(dataset.Xd.shape)\n",
    "print(dataset.Xg.shape)\n",
    "print(dataset.y.shape)\n",
    "print(dataset.basin.shape)\n",
    "print(dataset.time.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6edfcbb6-cb47-456f-b2c8-66aae58fb8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values: 0\n",
      "Proportion of NaN values: 0.0\n"
     ]
    }
   ],
   "source": [
    "# y_flat = dataset.y.flatten()\n",
    "# is_nan = np.isnan(y_flat)\n",
    "\n",
    "# # Count the number of NaN values by summing the boolean array\n",
    "# num_nan = np.sum(is_nan)\n",
    "# num = np.sum(y_flat)\n",
    "# print(num_nan)\n",
    "# print(num)\n",
    "# print(dataset.y)\n",
    "# Create a boolean array where True represents a NaN value\n",
    "is_nan = np.isnan(dataset.Xg)\n",
    "\n",
    "# Count the number of NaN values by summing the boolean array\n",
    "num_nan = np.sum(is_nan)\n",
    "\n",
    "# Calculate the proportion of NaN values\n",
    "proportion_nan = num_nan / np.size(dataset.Xd)\n",
    "\n",
    "print(\"Number of NaN values:\", num_nan)\n",
    "print(\"Proportion of NaN values:\", proportion_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13c37308-f228-4fab-add5-b4893024c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "\n",
    "def gen_loader(dataset, splits, **loader_args):\n",
    "    for train_idx, test_idx in splits:\n",
    "        train_loader = DataLoader(Subset(dataset, train_idx),\n",
    "                                  **loader_args, shuffle=True)\n",
    "        test_loader = DataLoader(Subset(dataset, test_idx),\n",
    "                                 **loader_args, shuffle=False)\n",
    "        yield train_loader, test_loader\n",
    "\n",
    "\n",
    "class BaseSplit:\n",
    "    def __init__(self, batch_size=32, num_workers=0, pin_memory=False) -> None:\n",
    "        super().__init__()\n",
    "        self.loader_args = {\n",
    "            'batch_size': batch_size,\n",
    "            'num_workers': num_workers,\n",
    "            'pin_memory': pin_memory\n",
    "        }\n",
    "\n",
    "    def __iter__(self):\n",
    "        return gen_loader(self.dataset, self.splits, **self.loader_args)\n",
    "    \n",
    "class TrainTestSplit(BaseSplit):\n",
    "    def __init__(self, dataset, batch_size=32, num_workers=0,\n",
    "                 pin_memory=False, test_size=0.2) -> None:\n",
    "        super().__init__(batch_size, num_workers, pin_memory)\n",
    "        # 获取每个样本的流域标签\n",
    "        basins = dataset.basin\n",
    "        # 唯一流域标签\n",
    "        unique_basins = np.unique(basins)\n",
    "        # 初始化训练和测试索引列表\n",
    "        train_idx, test_idx = [], []\n",
    "        # 对每个流域执行按比例划分\n",
    "        for basin in unique_basins:\n",
    "            # 当前流域的所有索引\n",
    "            idxs = np.where(basins == basin)[0]\n",
    "            # 计算训练集大小\n",
    "            train_size_int = int(len(idxs) * (1 - test_size))\n",
    "            # 根据比例划分当前流域的数据\n",
    "            # 这里按顺序划分，不随机\n",
    "            basin_train_idx = idxs[:train_size_int]\n",
    "            basin_test_idx = idxs[train_size_int:]\n",
    "            # 添加到总的训练和测试索引列表\n",
    "            train_idx.extend(basin_train_idx)\n",
    "            test_idx.extend(basin_test_idx)\n",
    "        # 转换为numpy数组\n",
    "        train_idx = np.array(train_idx)\n",
    "        test_idx = np.array(test_idx)\n",
    "        # 保存划分结果\n",
    "        self.dataset = dataset\n",
    "        self.splits = [(train_idx, test_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c154c1d9-e692-43ba-a8d0-2ae65aaf7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# def multi_arange(starts, stops, steps=1):\n",
    "#     lens = ((stops - starts) + steps - np.sign(steps)) // steps\n",
    "#     if isinstance(steps, np.ndarray):\n",
    "#         res = np.repeat(steps, lens)\n",
    "#     else:\n",
    "#         res = np.full(lens.sum(), steps)\n",
    "#     ends = (lens - 1) * steps + starts\n",
    "#     res[0] = starts[0]\n",
    "#     res[lens[:-1].cumsum()] = starts[1:] - ends[:-1]\n",
    "#     return res.cumsum()\n",
    "\n",
    "\n",
    "# def train_test_stratify(labels, test_size):\n",
    "#     bin_counts = np.bincount(labels)\n",
    "#     counts = bin_counts[np.nonzero(bin_counts)[0]]\n",
    "#     end = np.cumsum(counts)\n",
    "#     test_counts = np.rint(counts * test_size).astype('int')\n",
    "#     split = end - test_counts\n",
    "\n",
    "#     test_idx = multi_arange(split, end).astype(int)\n",
    "#     train_idx = np.setdiff1d(np.arange(len(labels)), test_idx)\n",
    "\n",
    "#     return train_idx, test_idx\n",
    "\n",
    "\n",
    "# class TrainTestSplit(BaseSplit):\n",
    "#     def __init__(self, dataset, batch_size=32, num_workers=0,\n",
    "#                  pin_memory=False, test_size=0.2) -> None:\n",
    "#         super().__init__(batch_size, num_workers, pin_memory)\n",
    "\n",
    "#         # Split dataset into train and test\n",
    "#         if test_size:\n",
    "#             split = train_test_stratify(dataset.basin, test_size)\n",
    "#             # split = train_test_split(\n",
    "#             #     np.arange(len(dataset.basin)),\n",
    "#             #     test_size=test_size,\n",
    "#             #     random_state=1,\n",
    "#             #     shuffle=False,\n",
    "#             #     stratify=dataset.basin)\n",
    "#             # print(split)\n",
    "#         else:\n",
    "#             split = (np.arange(len(dataset)), np.array([]))\n",
    "\n",
    "#         self.dataset = dataset\n",
    "#         self.splits = [split]\n",
    "\n",
    "class TrainTestSplit(BaseSplit):\n",
    "    def __init__(self, dataset, batch_size=32, num_workers=0,\n",
    "                 pin_memory=False, test_size=0.2) -> None:\n",
    "        super().__init__(batch_size, num_workers, pin_memory)\n",
    "\n",
    "        # 获取每个样本的流域标签\n",
    "        basins = dataset.basin\n",
    "        \n",
    "        # 唯一流域标签\n",
    "        unique_basins = np.unique(basins)\n",
    "        \n",
    "        # 初始化训练和测试索引列表\n",
    "        train_idx, test_idx = [], []\n",
    "\n",
    "        # 对每个流域执行按比例划分\n",
    "        for basin in unique_basins:\n",
    "            # 当前流域的所有索引\n",
    "            idxs = np.where(basins == basin)[0]\n",
    "            # 计算训练集大小\n",
    "            train_size_int = int(len(idxs) * (1 - test_size))\n",
    "            \n",
    "            # 根据比例划分当前流域的数据\n",
    "            # 这里按顺序划分，不随机\n",
    "            basin_train_idx = idxs[:train_size_int]\n",
    "            basin_test_idx = idxs[train_size_int:]\n",
    "            \n",
    "            # 添加到总的训练和测试索引列表\n",
    "            train_idx.extend(basin_train_idx)\n",
    "            test_idx.extend(basin_test_idx)\n",
    "\n",
    "        # 转换为numpy数组\n",
    "        train_idx = np.array(train_idx)\n",
    "        test_idx = np.array(test_idx)\n",
    "\n",
    "        # 保存划分结果\n",
    "        self.dataset = dataset\n",
    "        self.splits = [(train_idx, test_idx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b20d4179-0083-467d-8f4f-a06fad6b33fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([     0,      1,      2, ..., 120184, 120185, 120186]), array([   567,    568,    569, ..., 120278, 120279, 120280]))]\n"
     ]
    }
   ],
   "source": [
    "datasplit = TrainTestSplit(dataset, batch_size=32, num_workers=0,\n",
    "                 pin_memory=False, test_size=0.2)\n",
    "print(datasplit.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dca280f-2207-4651-af19-ee02c60641d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for train_loader, test_loader in datasplit:\n",
    "#     for batch_idx, (data, target, basin, time) in enumerate(test_loader):\n",
    "#         print(basin)\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6718253c-5d97-44cc-a885-15af30ce0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BALSTM(nn.Module):\n",
    "    def __init__(self, input_size_sta, input_size_dyn, input_size_glo, hidden_size, output_size, num_layers, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 使用SingleBALSTM作为子模块\n",
    "        self.lstm = SingleBALSTM(input_size_sta, input_size_dyn, input_size_glo, hidden_size,\n",
    "                                 batch_first=True, initial_forget_bias=0)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_s, x_d, x_g = data\n",
    "\n",
    "        # 前向传播LSTM\n",
    "        # out: tensor的形状为(batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.lstm(x_s, x_d, x_g)\n",
    "\n",
    "        # 解码最后一个时间步的隐藏状态\n",
    "        out = self.fc(self.dropout(out[:, -1, :]))\n",
    "\n",
    "        # 使用特定激活函数\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SingleBALSTM(nn.Module):\n",
    "    \"\"\"实现基于Basin-Aware-LSTM (BA-LSTM)的模型\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size_dyn : int\n",
    "        动态特征的数量，这些特征在每个时间步传递给LSTM。\n",
    "    input_size_sta : int\n",
    "        静态特征的数量，这些特征用于调制输入门。\n",
    "    input_size_glo : int\n",
    "        全局特征的数量，这些特征在每个时间步传递给LSTM。\n",
    "    hidden_size : int\n",
    "        隐藏/记忆单元的数量。\n",
    "    batch_first : bool, optional\n",
    "        如果为True，期望批量输入的形状为[batch, seq, features]；否则，形状应为[seq, batch, features]，默认为True。\n",
    "    initial_forget_bias : int, optional\n",
    "        初始遗忘门偏置的值，默认为0。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size_sta: int,\n",
    "                 input_size_dyn: int,\n",
    "                 input_size_glo: int,\n",
    "                 hidden_size: int,\n",
    "                 batch_first: bool = True,\n",
    "                 initial_forget_bias: int = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size_dyn = input_size_dyn\n",
    "        self.input_size_sta = input_size_sta\n",
    "        self.input_size_glo = input_size_glo\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        self.initial_forget_bias = initial_forget_bias\n",
    "\n",
    "        # 创建可学习参数的张量\n",
    "        self.weight_ih = nn.Parameter(torch.FloatTensor(input_size_dyn, 3 * hidden_size))   # 划分为三个门(f, d, o)\n",
    "        self.weight_hh = nn.Parameter(torch.FloatTensor(hidden_size, 3 * hidden_size))\n",
    "        self.weight_sh = nn.Parameter(torch.FloatTensor(input_size_sta, 2*hidden_size))  # 划分为两个门（i1，i2）\n",
    "        self.weight_gh = nn.Parameter(torch.FloatTensor(input_size_glo, hidden_size))   # 划分一个门（g）\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(3 * hidden_size))\n",
    "        self.bias_s = nn.Parameter(torch.FloatTensor(2 * hidden_size))\n",
    "        self.bias_g = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "        # 初始化参数\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"初始化LSTM的所有可学习参数\"\"\"\n",
    "        nn.init.orthogonal_(self.weight_ih.data)\n",
    "        nn.init.orthogonal_(self.weight_gh.data)\n",
    "        nn.init.orthogonal_(self.weight_sh)\n",
    "\n",
    "        weight_hh_data = torch.eye(self.hidden_size)\n",
    "        weight_hh_data = weight_hh_data.repeat(1, 3)\n",
    "        self.weight_hh.data = weight_hh_data\n",
    "\n",
    "        nn.init.constant_(self.bias.data, val=0)\n",
    "        nn.init.constant_(self.bias_s.data, val=0)\n",
    "\n",
    "        if self.initial_forget_bias != 0:\n",
    "            self.bias.data[:self.hidden_size] = self.initial_forget_bias\n",
    "\n",
    "    def forward(self, x_s: torch.Tensor, x_d: torch.Tensor, x_g: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"[summary]\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_d : torch.Tensor\n",
    "            包含动态特征序列的批量张量。形状必须与batch_first指定的格式匹配。\n",
    "        x_s : torch.Tensor\n",
    "            包含静态特征的批量张量。\n",
    "        x_g : torch.Tensor\n",
    "            包含全局特征的批量张量。\n",
    "        Returns\n",
    "        -------\n",
    "        h_n : torch.Tensor\n",
    "            批量中每个样本每个时间步的隐藏状态。\n",
    "        c_n : torch.Tensor\n",
    "            批量中每个样本每个时间步的记忆状态。\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            x_d = x_d.transpose(0, 1)\n",
    "            x_g = x_g.transpose(0, 1)   # (seq_len, batch_size, feature_size)            \n",
    "\n",
    "        seq_len, batch_size, _ = x_d.size()\n",
    "\n",
    "        h_0 = x_d.data.new(batch_size, self.hidden_size).zero_()\n",
    "        c_0 = x_d.data.new(batch_size, self.hidden_size).zero_()\n",
    "        h_x = (h_0, c_0)\n",
    "\n",
    "        # 用于临时存储所有中间隐藏/记忆状态的空列表\n",
    "        h_n, c_n = [], []\n",
    "\n",
    "        # 将偏置向量扩展到批量大小\n",
    "        bias_batch = (self.bias.unsqueeze(0).expand(batch_size, *self.bias.size()))\n",
    "        bias_g_batch = (self.bias_g.unsqueeze(0).expand(batch_size, *self.bias_g.size()))\n",
    "        # 仅计算输入门一次，因为输入是静态的\n",
    "        bias_s_batch = (self.bias_s.unsqueeze(0).expand(batch_size, *self.bias_s.size()))\n",
    "        i = (torch.addmm(bias_s_batch, x_s, self.weight_sh))\n",
    "        i1, i2 = i.chunk(2, 1)\n",
    "        i1 = torch.sigmoid(i1)\n",
    "        i2 = torch.sigmoid(i2)\n",
    "        \n",
    "        # 在输入序列上执行前向步骤\n",
    "        for t in range(seq_len):\n",
    "            h_0, c_0 = h_x\n",
    "\n",
    "            # 计算门\n",
    "            g = torch.addmm(bias_g_batch, x_g[t], self.weight_gh)\n",
    "            gates = (torch.addmm(bias_batch, h_0, self.weight_hh) +\n",
    "                     torch.mm(x_d[t], self.weight_ih))\n",
    "            f, d, o_h = gates.chunk(3, 1)\n",
    "            o = o_h + torch.mm(x_g[t], self.weight_gh)\n",
    "            c_1 = torch.sigmoid(f) * c_0 + i1 * torch.tanh(d) + i2 * torch.tanh(g)\n",
    "            h_1 = torch.sigmoid(o) * torch.tanh(c_1)\n",
    "\n",
    "            # 将中间隐藏/记忆状态存储在列表中\n",
    "            h_n.append(h_1)\n",
    "            c_n.append(c_1)\n",
    "\n",
    "            h_x = (h_1, c_1)\n",
    "\n",
    "        h_n = torch.stack(h_n, 0)\n",
    "        c_n = torch.stack(c_n, 0)\n",
    "\n",
    "        if self.batch_first:\n",
    "            h_n = h_n.transpose(0, 1)\n",
    "            c_n = c_n.transpose(0, 1)\n",
    "\n",
    "        return h_n, c_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "303ceb7b-b8b7-43ff-aae6-147a87102082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from abc import abstractmethod, ABCMeta\n",
    "from numpy import inf\n",
    "from utils.logger import EpochMetrics\n",
    "from pathlib import Path\n",
    "from hydra.utils import to_absolute_path\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import logging\n",
    "logger = logging.getLogger('base-trainer')\n",
    "\n",
    "class BaseTrainer(metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Base class for all trainers\n",
    "    \"\"\"\n",
    "    @ abstractmethod\n",
    "    def __init__(self, model, criterion, metric_ftns, optimizer, n_gpu, epochs, log_step, round):\n",
    "\n",
    "        # setup GPU device if available, move model into configured device\n",
    "        self.device, device_ids = self._prepare_device(n_gpu)\n",
    "        self.model = model.to(self.device)\n",
    "        if len(device_ids) > 1:\n",
    "            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.metric_ftns = metric_ftns\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.log_step = log_step\n",
    "\n",
    "        # setup metric monitoring for monitoring model performance and saving best-checkpoint\n",
    "        self.monitor = 'min loss/valid'\n",
    "\n",
    "        metric_names = ['loss'] + [type(met).__name__ for met in self.metric_ftns]\n",
    "        self.ep_metrics = EpochMetrics(metric_names, phases=('train', 'valid'), monitoring=self.monitor)\n",
    "        self.result_dir = f'epoch-results-{round}.csv' if round else f'epoch-results.csv'\n",
    "\n",
    "        self.checkpt_top_k = 3\n",
    "        self.early_stop = float('inf')\n",
    "\n",
    "        self.start_epoch = 1\n",
    "        self.checkpt_dir = Path('balstm/').absolute()\n",
    "        self.checkpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _train_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Training logic for an epoch\n",
    "\n",
    "        :param epoch: Current epoch number\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Full training logic\n",
    "        \"\"\"\n",
    "        not_improved_count = 0\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            result = self._train_epoch(epoch)\n",
    "            self.ep_metrics.update(epoch, result)\n",
    "\n",
    "            # print result metrics of this epoch\n",
    "            max_line_width = max(len(line) for line in str(self.ep_metrics).splitlines())\n",
    "            # divider ---\n",
    "            print('-' * max_line_width)\n",
    "            print(self.ep_metrics.latest().to_string(float_format=lambda x: f'{x:.4f}') + '\\n')\n",
    "\n",
    "            # check if model performance improved or not, for early stopping and topk saving\n",
    "            is_best = False\n",
    "            improved = self.ep_metrics.is_improved()\n",
    "            if improved:\n",
    "                not_improved_count = 0\n",
    "                is_best = True\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "\n",
    "            if not_improved_count > self.early_stop:\n",
    "                print(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                            \"Training stops.\".format(self.early_stop))\n",
    "                break\n",
    "\n",
    "            using_topk_save = self.checkpt_top_k > 0\n",
    "            self._save_checkpoint(epoch, save_best=is_best, save_latest=using_topk_save)\n",
    "\n",
    "            # keep top-k checkpoints only, using monitoring metrics\n",
    "            if using_topk_save:\n",
    "                self.ep_metrics.keep_topk_checkpt(self.checkpt_dir, self.checkpt_top_k)\n",
    "\n",
    "            self.ep_metrics.to_csv(self.result_dir)\n",
    "\n",
    "            # divider ===\n",
    "            print('=' * max_line_width)\n",
    "\n",
    "        # Return metric score for hyperparameter optimization\n",
    "        return self.ep_metrics.latest_metric()\n",
    "\n",
    "    def _prepare_device(self, n_gpu_use):\n",
    "        \"\"\"\n",
    "        setup GPU device if available, move model into configured device\n",
    "        \"\"\"\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        if n_gpu_use > 0 and n_gpu == 0:\n",
    "            logger.warning(\"Warning: There\\'s no GPU available on this machine,\"\n",
    "                           \"training will be performed on CPU.\")\n",
    "            n_gpu_use = 0\n",
    "        if n_gpu_use > n_gpu:\n",
    "            logger.warning(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available \"\n",
    "                           \"on this machine.\".format(n_gpu_use, n_gpu))\n",
    "            n_gpu_use = n_gpu\n",
    "        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
    "        list_ids = list(range(n_gpu_use))\n",
    "        print('Using device: {} | GPU count: {}'.format(device, n_gpu_use))\n",
    "        return device, list_ids\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, save_best=False, save_latest=True):\n",
    "        \"\"\"\n",
    "        Saving checkpoints\n",
    "\n",
    "        :param epoch: current epoch number\n",
    "        :param log: logging information of the epoch\n",
    "        :param save_best: if True, save a copy of current checkpoint file as 'model_best.pth'\n",
    "        :param save_latest: if True, save a copy of current checkpoint file as 'model_latest.pth'\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            'model': type(self.model).__name__,\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epoch_metrics': self.ep_metrics,\n",
    "        }\n",
    "        abs_path = self.checkpt_dir / 'checkpoint-epoch{}.pth'.format(epoch)\n",
    "        torch.save(state, abs_path)\n",
    "\n",
    "\n",
    "        rel_path = (\n",
    "            abs_path.relative_to(os.getcwd())  # 使用 os.getcwd() 替代 get_original_cwd()\n",
    "            if abs_path.is_relative_to(os.getcwd())\n",
    "            else abs_path\n",
    "        )\n",
    "        print(f\"Model checkpoint saved at: \\n    {rel_path}\")\n",
    "        if save_latest:\n",
    "            latest_path = self.checkpt_dir / 'model_latest.pth'\n",
    "            copyfile(abs_path, latest_path)\n",
    "        if save_best:\n",
    "            best_path = self.checkpt_dir / 'model_best.pth'\n",
    "            copyfile(abs_path, best_path)\n",
    "            rel_best_path = (best_path.relative_to(Path.cwd())\n",
    "                             if best_path.is_relative_to(Path.cwd())\n",
    "                             else best_path)\n",
    "            print(f\"Renewing best checkpoint: \\n    ...\\\\{rel_best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f909adb-494d-4c37-8a44-f218319fefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.utils import inf_loop\n",
    "from evaluate import evaluate\n",
    "from utils.logger import BatchMetrics\n",
    "\n",
    "\n",
    "\n",
    "class Trainer(BaseTrainer):\n",
    "    \"\"\"\n",
    "    Trainer class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, criterion, metric_ftns, optimizer, n_gpu, epochs, log_step, data_loader,\n",
    "                 valid_data_loader=None, lr_scheduler=None, len_epoch=None, round=0):\n",
    "        super().__init__(model, criterion, metric_ftns, optimizer, n_gpu, epochs, log_step, round)\n",
    "        self.data_loader = data_loader\n",
    "        if len_epoch is None:\n",
    "            # epoch-based training\n",
    "            self.len_epoch = len(self.data_loader)\n",
    "        else:\n",
    "            # iteration-based training\n",
    "            self.data_loader = inf_loop(data_loader)\n",
    "            self.len_epoch = len_epoch\n",
    "        self.valid_data_loader = valid_data_loader\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "        self.train_metrics = BatchMetrics('loss',\n",
    "                                          *[type(m).__name__ for m in self.metric_ftns],\n",
    "                                          postfix='/train')\n",
    "        self.valid_metrics = BatchMetrics('loss',\n",
    "                                          *[type(m).__name__ for m in self.metric_ftns],\n",
    "                                          postfix='/valid')\n",
    "    def _train_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Training logic for an epoch\n",
    "\n",
    "        :param epoch: Integer, current training epoch.\n",
    "        :return: A log that contains average loss and metric in this epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.train_metrics.reset()\n",
    "        outputs = []\n",
    "        targets = []\n",
    "        labels = []\n",
    "        for batch_idx, (data, target, basin, time) in enumerate(self.data_loader):\n",
    "            target = target.to(self.device)\n",
    "            if isinstance(data, torch.Tensor):\n",
    "                data = data.to(self.device)\n",
    "            else:\n",
    "                data = tuple(i.to(self.device) for i in data)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target, basin)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # step = (epoch - 1) * self.len_epoch + batch_idx\n",
    "            self.train_metrics.update('loss', loss.item())\n",
    "            outputs.append(output)\n",
    "            targets.append(target)\n",
    "            labels.append(basin)\n",
    "                       \n",
    "\n",
    "            if batch_idx % self.log_step == 0:\n",
    "                print('Train Epoch: {} {} Loss: {:.6f}'.format(\n",
    "                    epoch,\n",
    "                    self._progress(batch_idx),\n",
    "                    loss.item()))\n",
    "\n",
    "            if batch_idx == self.len_epoch:\n",
    "                break\n",
    "        \n",
    "        output = torch.cat(outputs)\n",
    "        target = torch.cat(targets)\n",
    "        basin = torch.cat(labels)    \n",
    "        for metric_ftn in self.metric_ftns:\n",
    "            metric_value = metric_ftn(output, target, basin).cpu().detach().numpy()\n",
    "            self.train_metrics.update(type(metric_ftn).__name__, metric_value)            \n",
    "        \n",
    "        \n",
    "        # 添加训练期度量指标\n",
    "        # metrics1 = evaluate(self.model, self.data_loader, *self.metric_ftns)\n",
    "        # for metrict_ftn, metric1 in zip(self.metric_ftns, metrics1):\n",
    "        #     self.train_metrics.update(type(metrict_ftn).__name__, metric1)\n",
    "\n",
    "        log = self.train_metrics.result()\n",
    "        \n",
    "\n",
    "        if (self.valid_data_loader is not None and\n",
    "                len(self.valid_data_loader) > 0):\n",
    "            val_log = self._valid_epoch(epoch)\n",
    "            log.update(**val_log)\n",
    "\n",
    "        if self.lr_scheduler is not None:\n",
    "            self.lr_scheduler.step()\n",
    "        \n",
    "        # # 打印整个训练周期的结果度量\n",
    "        # for k, v in log.items():\n",
    "        #     print(f\"Epoch: {epoch}, Metric: {k}, Value: {v}\")\n",
    "\n",
    "        return log\n",
    "\n",
    "    def _valid_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Validate after training an epoch\n",
    "\n",
    "        :param epoch: Integer, current training epoch.\n",
    "        :return: A log that contains information about validation\n",
    "        \"\"\"\n",
    "        self.valid_metrics.reset()\n",
    "        metrics = evaluate(self.model, self.valid_data_loader, [self.criterion, *self.metric_ftns])\n",
    "        self.valid_metrics.update('loss', metrics[0])\n",
    "        for metrict_ftn, metric in zip(self.metric_ftns, metrics[1:]):\n",
    "            self.valid_metrics.update(type(metrict_ftn).__name__, metric)\n",
    "\n",
    "        # add histogram of model parameters to the tensorboard\n",
    "        # for name, p in self.model.named_parameters():\n",
    "        #     self.writer.add_histogram(name, p, bins='auto')\n",
    "        return self.valid_metrics.result()\n",
    "\n",
    "    def _progress(self, batch_idx):\n",
    "        base = '[{}/{} ({:.0f}%)]'\n",
    "        try:\n",
    "            # epoch-based training\n",
    "            total = len(self.data_loader.dataset)\n",
    "            current = batch_idx * self.data_loader.batch_size\n",
    "        except AttributeError:\n",
    "            # iteration-based training\n",
    "            total = self.len_epoch\n",
    "            current = batch_idx\n",
    "        return base.format(current, total, 100.0 * current / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9630770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5891eb6e-943b-4060-aed3-768dfc59ebd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0 | GPU count: 1\n",
      "Train Epoch: 1 [0/96155 (0%)] Loss: 36.053913\n",
      "Train Epoch: 1 [32000/96155 (33%)] Loss: 1.790355\n",
      "Train Epoch: 1 [64000/96155 (67%)] Loss: 2.193565\n",
      "Train Epoch: 1 [96000/96155 (100%)] Loss: 1.947436\n",
      "-----------------------------------------------------------------------\n",
      "           loss            NSE          MeanNSE         MedianNSE        \n",
      "          train  valid   train   valid    train   valid     train   valid\n",
      "epoch-1 15.3873 1.6053 -0.5551 -0.3749 -25.9265 -3.1005   -4.5043 -1.3173\n",
      "\n",
      "Model checkpoint saved at: \n",
      "    balstm\\checkpoint-epoch1.pth\n",
      "Renewing best checkpoint: \n",
      "    ...\\balstm\\model_best.pth\n",
      "=======================================================================\n",
      "Train Epoch: 2 [0/96155 (0%)] Loss: 1.453934\n",
      "Train Epoch: 2 [32000/96155 (33%)] Loss: 2.353580\n",
      "Train Epoch: 2 [64000/96155 (67%)] Loss: 1.218425\n"
     ]
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "from trainer.loss import NMSELoss\n",
    "from trainer.metric import *\n",
    "import importlib\n",
    "\n",
    "n_gpu = 1\n",
    "epochs = 100\n",
    "log_step = 1000\n",
    "milestones = [700]\n",
    "# 定义衰减比例\n",
    "gamma = 0.5\n",
    "learning_rate = 0.0001\n",
    "n, accumulation = 0, 0\n",
    "for train_loader, test_loader in datasplit:\n",
    "    # build model and print its architecture\n",
    "    model = BALSTM(input_size_sta, input_size_dyn, input_size_glo, hidden_size = 64,\n",
    "                    output_size = 1, num_layers = 2, drop_prob = 0.4)\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    criterion = NMSELoss(dataset)\n",
    "   # 定义度量函数的完全限定名\n",
    "    metrics_fqns = [\n",
    "        \"trainer.metric.NSE\",\n",
    "        \"trainer.metric.MeanNSE\",\n",
    "        \"trainer.metric.MedianNSE\",\n",
    "    ]\n",
    "    # 实例化度量函数\n",
    "    metrics = []\n",
    "    for fqn in metrics_fqns:\n",
    "        module_name, class_name = fqn.rsplit(\".\", 1)\n",
    "        MetricClass = getattr(importlib.import_module(module_name), class_name)\n",
    "        metrics.append(MetricClass())\n",
    "    metrics = [met for met in metrics]\n",
    "    # build optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "    # train and accumulate metric\n",
    "    trainer = Trainer(model, criterion, metrics, optimizer, n_gpu, epochs, log_step, \n",
    "                      data_loader=train_loader, valid_data_loader=test_loader, \n",
    "                      lr_scheduler=lr_scheduler, round=n)\n",
    "    accumulation += trainer.train()\n",
    "    n += 1\n",
    "print(accumulation / n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518abbc-fb2d-43a1-b527-302aa900ef5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for input, target, label in test_loader:\n",
    "#     print(len(label))\n",
    "#     # print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a77801e-ce1b-4412-91f5-a8c2ee24c178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "for train_loader, test_loader in datasplit:\n",
    "    for batch_idx, (data, target, basin, time) in enumerate(train_loader):\n",
    "        for tensor in data:\n",
    "            is_nan = torch.isnan(tensor)\n",
    "            if torch.any(is_nan):\n",
    "                print(\"Found a NaN value!\")\n",
    "# Calculate the proportion of NaN values\n",
    "#         proportion_nan = num_nan / np.size(dataset.y)\n",
    "\n",
    "#         print(\"Number of NaN values:\", num_nan)\n",
    "#         print(\"Proportion of NaN values:\", proportion_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddaf46-1440-4129-ae94-b26759e7b03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from trainer.metric import *\n",
    "\n",
    "# Assuming you have the normalize function defined\n",
    "def inverse_normalize(data, mean, std):\n",
    "    return data * std + mean\n",
    "\n",
    "_, _, _, _, _, _, y_mean, y_std = dataset.norm\n",
    "pth = torch.load('balstm/model_latest.pth')\n",
    "test_loader = next(iter(datasplit))[1]\n",
    "# load trained weights\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(pth['state_dict'])\n",
    "# prepare model for testing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# simulate\n",
    "with torch.no_grad():\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    times = []\n",
    "    for input, target, label, time in test_loader:\n",
    "        \n",
    "        target = target.to(device)\n",
    "        if isinstance(input, torch.Tensor):\n",
    "            input = input.to(device)\n",
    "        else:\n",
    "            input = tuple(i.to(device) for i in input)\n",
    "        outputs.append(model(input))\n",
    "        targets.append(target)\n",
    "        labels.append(label)\n",
    "        times.append(time)\n",
    "    \n",
    "    # concatenate in dim 0\n",
    "    y_pred = torch.cat(outputs)\n",
    "    y_true = torch.cat(targets)\n",
    "    basin = torch.cat(labels)\n",
    "    Time = torch.cat(times)\n",
    "\n",
    "mask = ~torch.isnan(y_true)  # Create a mask to exclude NaN values\n",
    "y_pred = y_pred[mask]\n",
    "y_true = y_true[mask]\n",
    "mask = mask.flatten().cpu()\n",
    "basin = basin[mask]\n",
    "Time = Time[mask]\n",
    "# 反归一化 y_true and y_pred\n",
    "y_true_denorm = inverse_normalize(y_true.cpu().numpy(), y_mean, y_std)\n",
    "y_pred_denorm = inverse_normalize(y_pred.cpu().numpy(), y_mean, y_std)\n",
    "# 创建一个向量化函数\n",
    "fromordinal_vectorized = np.vectorize(lambda ordinal: datetime.datetime.fromordinal(ordinal).date())\n",
    "# 使用向量化函数转换时间\n",
    "Time = fromordinal_vectorized(Time)\n",
    "\n",
    "nse_calculator = SingleNSE()\n",
    "df1 = pd.DataFrame()\n",
    "df = pd.DataFrame()\n",
    "df['STAID'] = dataset.gages['STAID']\n",
    "df['KGE'] = seqKGE(y_pred, y_true, basin).cpu().numpy()\n",
    "df['NSE'] = nse_calculator(y_pred, y_true, basin).cpu().numpy()\n",
    "df1['basin'] = basin.cpu().numpy().ravel()\n",
    "df1['y_true'] = y_true_denorm.ravel()\n",
    "df1['y_pred'] = y_pred_denorm.ravel()\n",
    "df1['time'] = Time.ravel()\n",
    "df.to_csv('basin-results6.csv', index=None)\n",
    "df1.to_csv('basin-data6.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00699995-449e-4a83-a9a1-294671afbc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(df['NSE'].mean())\n",
    "# print(y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae797b28-24c1-42a1-8da3-860a8508795a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import hydra\n",
    "# from hydra.utils import instantiate\n",
    "# from trainer.loss import NMSELoss\n",
    "# from trainer import Trainer\n",
    "# import logging\n",
    "# from hydra.experimental import compose, initialize\n",
    "\n",
    "# def main(cfg):\n",
    "\n",
    "#     logger = logging.getLogger('trainer')\n",
    "#     # 定义里程碑，例如在第700个epoch时改变学习率\n",
    "#     milestones = [700]\n",
    "#     # 定义衰减比例\n",
    "#     gamma = 0.5\n",
    "#     learning_rate = 0.0001\n",
    "#     n, accumulation = 0, 0\n",
    "#     for train_loader, test_loader in datasplit:\n",
    "#         # build model and print its architecture\n",
    "#         model = BALSTM(input_size_sta, input_size_dyn, input_size_glo, hidden_size = 128,\n",
    "#                        output_size = 1, num_layers = 1, drop_prob = 0.4)\n",
    "#         trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "#         logger.info(model)\n",
    "#         logger.info(f'Trainable parameters: {sum([p.numel() for p in trainable_params])}')\n",
    "\n",
    "#         # get function handles of loss and metrics\n",
    "#         criterion = NMSELoss(dataset)\n",
    "#         metrics = [instantiate(met) for met in cfg.trainer['metrics']]\n",
    "#         # build optimizer and learning rate scheduler\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "#         # train and accumulate metric\n",
    "#         trainer = Trainer(model, criterion, metrics, optimizer,\n",
    "#                             config = cfg,\n",
    "#                             data_loader=train_loader,\n",
    "#                             valid_data_loader=test_loader,\n",
    "#                             lr_scheduler=lr_scheduler,\n",
    "#                             round=n)\n",
    "#         accumulation += trainer.train()\n",
    "#         n += 1\n",
    "#     return accumulation / n\n",
    "# if __name__ == \"__main__\":\n",
    "#     initialize(config_path='configs')\n",
    "#     cfg = compose(config_name='config')\n",
    "#     main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchhydro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
